<!DOCTYPE html>
<html>

<head>
  <title>Blog</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css">









  <style>
    /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
    code[class*="language-"],
    pre[class*="language-"] {
      color: #333;
      background: none;
      font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
      text-align: left;
      white-space: pre;
      word-spacing: normal;
      word-break: normal;
      word-wrap: normal;
      line-height: 1.4;

      -moz-tab-size: 8;
      -o-tab-size: 8;
      tab-size: 8;

      -webkit-hyphens: none;
      -moz-hyphens: none;
      -ms-hyphens: none;
      hyphens: none;
    }

    /* Code blocks */
    pre[class*="language-"] {
      padding: .8em;
      overflow: auto;
      /* border: 1px solid #ddd; */
      border-radius: 3px;
      /* background: #fff; */
      background: #f5f5f5;
    }

    /* Inline code */
    :not(pre)>code[class*="language-"] {
      padding: .1em;
      border-radius: .3em;
      white-space: normal;
      background: #f5f5f5;
    }

    .token.comment,
    .token.blockquote {
      color: #969896;
    }

    .token.cdata {
      color: #183691;
    }

    .token.doctype,
    .token.punctuation,
    .token.variable,
    .token.macro.property {
      color: #333;
    }

    .token.operator,
    .token.important,
    .token.keyword,
    .token.rule,
    .token.builtin {
      color: #a71d5d;
    }

    .token.string,
    .token.url,
    .token.regex,
    .token.attr-value {
      color: #183691;
    }

    .token.property,
    .token.number,
    .token.boolean,
    .token.entity,
    .token.atrule,
    .token.constant,
    .token.symbol,
    .token.command,
    .token.code {
      color: #0086b3;
    }

    .token.tag,
    .token.selector,
    .token.prolog {
      color: #63a35c;
    }

    .token.function,
    .token.namespace,
    .token.pseudo-element,
    .token.class,
    .token.class-name,
    .token.pseudo-class,
    .token.id,
    .token.url-reference .token.variable,
    .token.attr-name {
      color: #795da3;
    }

    .token.entity {
      cursor: help;
    }

    .token.title,
    .token.title .token.punctuation {
      font-weight: bold;
      color: #1d3e81;
    }

    .token.list {
      color: #ed6a43;
    }

    .token.inserted {
      background-color: #eaffea;
      color: #55a532;
    }

    .token.deleted {
      background-color: #ffecec;
      color: #bd2c00;
    }

    .token.bold {
      font-weight: bold;
    }

    .token.italic {
      font-style: italic;
    }


    /* JSON */
    .language-json .token.property {
      color: #183691;
    }

    .language-markup .token.tag .token.punctuation {
      color: #333;
    }

    /* CSS */
    code.language-css,
    .language-css .token.function {
      color: #0086b3;
    }

    /* YAML */
    .language-yaml .token.atrule {
      color: #63a35c;
    }

    code.language-yaml {
      color: #183691;
    }

    /* Ruby */
    .language-ruby .token.function {
      color: #333;
    }

    /* Markdown */
    .language-markdown .token.url {
      color: #795da3;
    }

    /* Makefile */
    .language-makefile .token.symbol {
      color: #795da3;
    }

    .language-makefile .token.variable {
      color: #183691;
    }

    .language-makefile .token.builtin {
      color: #0086b3;
    }

    /* Bash */
    .language-bash .token.keyword {
      color: #0086b3;
    }

    /* highlight */
    pre[data-line] {
      position: relative;
      padding: 1em 0 1em 3em;
    }

    pre[data-line] .line-highlight-wrapper {
      position: absolute;
      top: 0;
      left: 0;
      background-color: transparent;
      display: block;
      width: 100%;
    }

    pre[data-line] .line-highlight {
      position: absolute;
      left: 0;
      right: 0;
      padding: inherit 0;
      margin-top: 1em;
      background: hsla(24, 20%, 50%, .08);
      background: linear-gradient(to right, hsla(24, 20%, 50%, .1) 70%, hsla(24, 20%, 50%, 0));
      pointer-events: none;
      line-height: inherit;
      white-space: pre;
    }

    pre[data-line] .line-highlight:before,
    pre[data-line] .line-highlight[data-end]:after {
      content: attr(data-start);
      position: absolute;
      top: .4em;
      left: .6em;
      min-width: 1em;
      padding: 0 .5em;
      background-color: hsla(24, 20%, 50%, .4);
      color: hsl(24, 20%, 95%);
      font: bold 65%/1.5 sans-serif;
      text-align: center;
      vertical-align: .3em;
      border-radius: 999px;
      text-shadow: none;
      box-shadow: 0 1px white;
    }

    pre[data-line] .line-highlight[data-end]:after {
      content: attr(data-end);
      top: auto;
      bottom: .4em;
    }

    html body {
      font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
      font-size: 16px;
      line-height: 1.6;
      color: #333;
      background-color: #fff;
      overflow: initial;
      box-sizing: border-box;
      word-wrap: break-word
    }

    html body>:first-child {
      margin-top: 0
    }

    html body h1,
    html body h2,
    html body h3,
    html body h4,
    html body h5,
    html body h6 {
      line-height: 1.2;
      margin-top: 1em;
      margin-bottom: 16px;
      color: #000
    }

    html body h1 {
      font-size: 2.25em;
      font-weight: 300;
      padding-bottom: .3em
    }

    html body h2 {
      font-size: 1.75em;
      font-weight: 400;
      padding-bottom: .3em
    }

    html body h3 {
      font-size: 1.5em;
      font-weight: 500
    }

    html body h4 {
      font-size: 1.25em;
      font-weight: 600
    }

    html body h5 {
      font-size: 1.1em;
      font-weight: 600
    }

    html body h6 {
      font-size: 1em;
      font-weight: 600
    }

    html body h1,
    html body h2,
    html body h3,
    html body h4,
    html body h5 {
      font-weight: 600
    }

    html body h5 {
      font-size: 1em
    }

    html body h6 {
      color: #5c5c5c
    }

    html body strong {
      color: #000
    }

    html body del {
      color: #5c5c5c
    }

    html body a:not([href]) {
      color: inherit;
      text-decoration: none
    }

    html body a {
      color: #08c;
      text-decoration: none
    }

    html body a:hover {
      color: #00a3f5;
      text-decoration: none
    }

    html body img {
      max-width: 100%
    }

    html body>p {
      margin-top: 0;
      margin-bottom: 16px;
      word-wrap: break-word
    }

    html body>ul,
    html body>ol {
      margin-bottom: 16px
    }

    html body ul,
    html body ol {
      padding-left: 2em
    }

    html body ul.no-list,
    html body ol.no-list {
      padding: 0;
      list-style-type: none
    }

    html body ul ul,
    html body ul ol,
    html body ol ol,
    html body ol ul {
      margin-top: 0;
      margin-bottom: 0
    }

    html body li {
      margin-bottom: 0
    }

    html body li.task-list-item {
      list-style: none
    }

    html body li>p {
      margin-top: 0;
      margin-bottom: 0
    }

    html body .task-list-item-checkbox {
      margin: 0 .2em .25em -1.8em;
      vertical-align: middle
    }

    html body .task-list-item-checkbox:hover {
      cursor: pointer
    }

    html body blockquote {
      margin: 16px 0;
      font-size: inherit;
      padding: 0 15px;
      color: #5c5c5c;
      background-color: #f0f0f0;
      border-left: 4px solid #d6d6d6
    }

    html body blockquote>:first-child {
      margin-top: 0
    }

    html body blockquote>:last-child {
      margin-bottom: 0
    }

    html body hr {
      height: 4px;
      margin: 32px 0;
      background-color: #d6d6d6;
      border: 0 none
    }

    html body table {
      margin: 10px 0 15px 0;
      border-collapse: collapse;
      border-spacing: 0;
      display: block;
      width: 100%;
      overflow: auto;
      word-break: normal;
      word-break: keep-all
    }

    html body table th {
      font-weight: bold;
      color: #000
    }

    html body table td,
    html body table th {
      border: 1px solid #d6d6d6;
      padding: 6px 13px
    }

    html body dl {
      padding: 0
    }

    html body dl dt {
      padding: 0;
      margin-top: 16px;
      font-size: 1em;
      font-style: italic;
      font-weight: bold
    }

    html body dl dd {
      padding: 0 16px;
      margin-bottom: 16px
    }

    html body code {
      font-family: Menlo, Monaco, Consolas, 'Courier New', monospace;
      font-size: .85em !important;
      color: #000;
      background-color: #f0f0f0;
      border-radius: 3px;
      padding: .2em 0
    }

    html body code::before,
    html body code::after {
      letter-spacing: -0.2em;
      content: "\00a0"
    }

    html body pre>code {
      padding: 0;
      margin: 0;
      font-size: .85em !important;
      word-break: normal;
      white-space: pre;
      background: transparent;
      border: 0
    }

    html body .highlight {
      margin-bottom: 16px
    }

    html body .highlight pre,
    html body pre {
      padding: 1em;
      overflow: auto;
      font-size: .85em !important;
      line-height: 1.45;
      border: #d6d6d6;
      border-radius: 3px
    }

    html body .highlight pre {
      margin-bottom: 0;
      word-break: normal
    }

    html body pre code,
    html body pre tt {
      display: inline;
      max-width: initial;
      padding: 0;
      margin: 0;
      overflow: initial;
      line-height: inherit;
      word-wrap: normal;
      background-color: transparent;
      border: 0
    }

    html body pre code:before,
    html body pre tt:before,
    html body pre code:after,
    html body pre tt:after {
      content: normal
    }

    html body p,
    html body blockquote,
    html body ul,
    html body ol,
    html body dl,
    html body pre {
      margin-top: 0;
      margin-bottom: 16px
    }

    html body kbd {
      color: #000;
      border: 1px solid #d6d6d6;
      border-bottom: 2px solid #c7c7c7;
      padding: 2px 4px;
      background-color: #f0f0f0;
      border-radius: 3px
    }

    @media print {
      html body {
        background-color: #fff
      }

      html body h1,
      html body h2,
      html body h3,
      html body h4,
      html body h5,
      html body h6 {
        color: #000;
        page-break-after: avoid
      }

      html body blockquote {
        color: #5c5c5c
      }

      html body pre {
        page-break-inside: avoid
      }

      html body table {
        display: table
      }

      html body img {
        display: block;
        max-width: 100%;
        max-height: 100%
      }

      html body pre,
      html body code {
        word-wrap: break-word;
        white-space: pre
      }
    }

    .markdown-preview {
      width: 100%;
      height: 100%;
      box-sizing: border-box
    }

    .markdown-preview .pagebreak,
    .markdown-preview .newpage {
      page-break-before: always
    }

    .markdown-preview pre.line-numbers {
      position: relative;
      padding-left: 3.8em;
      counter-reset: linenumber
    }

    .markdown-preview pre.line-numbers>code {
      position: relative
    }

    .markdown-preview pre.line-numbers .line-numbers-rows {
      position: absolute;
      pointer-events: none;
      top: 1em;
      font-size: 100%;
      left: 0;
      width: 3em;
      letter-spacing: -1px;
      border-right: 1px solid #999;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none
    }

    .markdown-preview pre.line-numbers .line-numbers-rows>span {
      pointer-events: none;
      display: block;
      counter-increment: linenumber
    }

    .markdown-preview pre.line-numbers .line-numbers-rows>span:before {
      content: counter(linenumber);
      color: #999;
      display: block;
      padding-right: .8em;
      text-align: right
    }

    .markdown-preview .mathjax-exps .MathJax_Display {
      text-align: center !important
    }

    .markdown-preview:not([for="preview"]) .code-chunk .btn-group {
      display: none
    }

    .markdown-preview:not([for="preview"]) .code-chunk .status {
      display: none
    }

    .markdown-preview:not([for="preview"]) .code-chunk .output-div {
      margin-bottom: 16px
    }

    .scrollbar-style::-webkit-scrollbar {
      width: 8px
    }

    .scrollbar-style::-webkit-scrollbar-track {
      border-radius: 10px;
      background-color: transparent
    }

    .scrollbar-style::-webkit-scrollbar-thumb {
      border-radius: 5px;
      background-color: rgba(150, 150, 150, 0.66);
      border: 4px solid rgba(150, 150, 150, 0.66);
      background-clip: content-box
    }

    html body[for="html-export"]:not([data-presentation-mode]) {
      position: relative;
      width: 100%;
      height: 100%;
      top: 0;
      left: 0;
      margin: 0;
      padding: 0;
      overflow: auto
    }

    html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview {
      position: relative;
      top: 0
    }

    @media screen and (min-width:914px) {
      html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview {
        padding: 2em calc(50% - 457px + 2em)
      }
    }

    @media screen and (max-width:914px) {
      html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview {
        padding: 2em
      }
    }

    @media screen and (max-width:450px) {
      html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview {
        font-size: 14px !important;
        padding: 1em
      }
    }

    @media print {
      html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn {
        display: none
      }
    }

    html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn {
      position: fixed;
      bottom: 8px;
      left: 8px;
      font-size: 28px;
      cursor: pointer;
      color: inherit;
      z-index: 99;
      width: 32px;
      text-align: center;
      opacity: .4
    }

    html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn {
      opacity: 1
    }

    html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc {
      position: fixed;
      top: 0;
      left: 0;
      width: 300px;
      height: 100%;
      padding: 32px 0 48px 0;
      font-size: 14px;
      box-shadow: 0 0 4px rgba(150, 150, 150, 0.33);
      box-sizing: border-box;
      overflow: auto;
      background-color: inherit
    }

    html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar {
      width: 8px
    }

    html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track {
      border-radius: 10px;
      background-color: transparent
    }

    html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb {
      border-radius: 5px;
      background-color: rgba(150, 150, 150, 0.66);
      border: 4px solid rgba(150, 150, 150, 0.66);
      background-clip: content-box
    }

    html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a {
      text-decoration: none
    }

    html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul {
      padding: 0 1.6em;
      margin-top: .8em
    }

    html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc li {
      margin-bottom: .8em
    }

    html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul {
      list-style-type: none
    }

    html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview {
      left: 300px;
      width: calc(100% - 300px);
      padding: 2em calc(50% - 457px - 150px);
      margin: 0;
      box-sizing: border-box
    }

    @media screen and (max-width:1274px) {
      html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview {
        padding: 2em
      }
    }

    @media screen and (max-width:450px) {
      html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview {
        width: 100%
      }
    }

    html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview {
      left: 50%;
      transform: translateX(-50%)
    }

    html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc {
      display: none
    }

    .center {
      display: block;
      margin-left: auto;
      margin-right: auto;
      width: 50%;
    }
  </style>
</head>

<body for="html-export">
  <div class="mume markdown-preview  ">
    <h1 class="mume-header" id="facial-expression-recognition-using-pytorch">Facial Expression Recognition using PyTorch
    </h1>

    <p><img src="cover.png" alt="Cover image" class="center"></p>
    <p>Hello everyone, I hope you are doing well during these time. In this post, we&apos;re going to look at how to go
      on about building a <strong>facial expression recognition</strong> project from scratch using PyTorch. In this
      post we&apos;ll start from simple task such as downloading dataset, dataset preparation to writing our own custom
      CNN and a build a ResNet-9 for our use case. We&apos;ll also experiment with differnt learning rate schedulers.
      This article assumes you the basic knowledge of PyTorch. Hope you enjoy it as much as I did while doing it.</p>
    <p>This post is a part of my final project for the free course provided by <a
        href="https://jovian.ml/forum/t/start-here-welcome-to-deep-learning-with-pytorch-zero-to-gans/1622">jovian.ml -
        Pytorch: zero to gans</a>.</p>
    <h2 class="mume-header" id="table-of-content">Table of Content</h2>

    <ol>
      <li>Introduction</li>
      <li>Importing modules</li>
      <li>Dataset Preparation</li>
      <li>Augmentaions</li>
      <li>Building Dataloader</li>
      <li>Setup GPU</li>
      <li>Model Building
        <ol>
          <li>Parent Class and Metrics</li>
          <li>Models
            <ul>
              <li>ResNet9</li>
              <li>Custom CNN model</li>
            </ul>
          </li>
        </ol>
      </li>
      <li>Training and Testing Helper Functions</li>
      <li>Training</li>
      <li>Training plots</li>
      <li>Tests</li>
      <li>Summary</li>
    </ol>
    <h3 class="mume-header" id="1-introduction">1. Introduction</h3>

    <p>The <strong>goal</strong> of Facial Expression Recognition is to classify the expressions on face images into
      various categories such as anger, fear, surprise, sadness, happiness and so on. Specifically we are going to
      classify are datasets into 7 categories such as:</p>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=6"
      title="Jovian Viewer" height="248" width="850" frameborder="0" scrolling="auto"></iframe>
    <p>The dataset used for this project can be downloaded from <a
        href="https://www.kaggle.com/ashishpatel26/fer2018">here</a>. This dataset consists of <code>48x48</code> pixel
      grayscale images of faces. The faces have been automatically registered so that the face is more or less centered
      and occupies about the same amount of space in each image.</p>
    <h3 class="mume-header" id="2-importing-modules">2. Importing modules</h3>

    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=4"
      title="Jovian Viewer" height="668" width="850" frameborder="0" scrolling="auto"></iframe>
    <h3 class="mume-header" id="3-dataset-preparation">3. Dataset Preparation</h3>

    <p>Now that we know the goal of this project, let&apos;s start with the very first step of any machine learning
      project i.e. preparing our dataset. After downloading the data, we can see that our data is in a <code>.csv</code>
      file. To use this .csv file we can use the <code>Pandas</code> function <strong>read_csv(..)</strong> to load the
      data into memory.</p>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=7"
      title="Jovian Viewer" height="357" width="850" frameborder="0" scrolling="auto"></iframe>
    <p>We can see that our .csv file contains 3 columns:</p>
    <ol>
      <li><strong>emotion</strong> - contains integer values representing one of 7 emotions.</li>
      <li><strong>pixels</strong> - each row contains a string of pixel values seperated by spaces 2304 (48x48) pixel
        values.</li>
      <li><strong>Usage</strong> - it contains 3 unique values <code>Traininig</code>, <code>PrivateTest</code>,
        <code>PublicTest</code>. It states which rows are to be used for training and testing purpose.</li>
    </ol>
    <p>We&apos;re going to combine together the <code>Training</code> and <code>PublicTest</code> rows and use them as
      <strong>training and validation set</strong>, splitted in an <strong>80-20</strong> proportion. We&apos;ll use the
      <code>PrivateTest</code> rows as our <strong>test</strong> dataset.</p>
    <p>Taking a quick peek at the <code>emotion</code> column we can have see that there is heaby class imbalance in our
      dataset. To remedy this, we&apos;ll use different data transformations techniques available in pytorch.</p>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=16&amp;hideInput=true"
      title="Jovian Viewer" height="550" width="850" frameborder="0" scrolling="auto"></iframe>
    <p>One more thing what we can do is merge the <code>Angry</code> and <code>Disgust</code> class into one as both as
      highly related.<br>
      We can do so by:</p>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=17"
      title="Jovian Viewer" height="185" width="850" frameborder="0" scrolling="auto"></iframe>
    <p>Now, update the emotions dictionary.</p>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=18"
      title="Jovian Viewer" height="268" width="850" frameborder="0" scrolling="auto"></iframe>
    <p>Let&apos;s first extract the pixel values for each row from <strong>pixels</strong> column.<br>
      <strong>Steps:</strong></p>
    <ol>
      <li>Iterate over each row to access the <em>str</em> value.</li>
      <li>use the <em>.split()</em> method to split the string on spaces creating a list.</li>
      <li>Iterate over each value in the list convert them to <em>int</em> and then finally append the whole list of
        integers in a list named <strong>pixels</strong>.</li>
      <li>Convert the list to a numpy array.</li>
      <li>Rescale the pixel values by dividing by <strong>255.0</strong></li>
      <li>Drop the original pixels column.</li>
      <li>Add the pixels array created as a column back in the dataset.</li>
    </ol>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=11"
      title="Jovian Viewer" height="588" width="850" frameborder="0" scrolling="auto"></iframe>
    <p>Doing so our dataset dataframe object will now contain <strong>2306 rows</strong>, with 2304 columns for
      <em>pixels</em>, 1 for <em>emotion</em> and 1 for <em>Usage</em>.</p>
    <p>Finally now we can now create our <strong>cusotm Dataset</strong> class that will come in handy for applying data
      augmentation and for building our dataloaders. Here we are simply defining methods that will on each call by the
      <strong>Dataloader</strong> help in retreiving the labels <strong>y</strong> i.e. the emotion and the pixel values
      i.e. our <strong>X</strong> values reshaped as a 48x48 matrix as a single tuple of <strong>(X, y)</strong>. Doing
      this, we can freely define our augmentation methods without worrying about the implementation details as the
      augmentation for each image can be directly applied during a call.</p>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=14"
      title="Jovian Viewer" height="585" width="850" frameborder="0" scrolling="auto"></iframe>
    <h3 class="mume-header" id="4-augmentations">4. Augmentations</h3>

    <p>There&apos;s a whole list of different augmenations we can apply to our images they can be found <a
        href="https://pytorch.org/docs/stable/torchvision/transforms.html">here.</a></p>
    <p>For this project I chose to go with some basic augmentaions such as:</p>
    <ol>
      <li>RandomCrop: It is used to crop the PIL image at a random location.</li>
      <li>RandomRotation: rotate the image.</li>
      <li>RandomAffine: it applies a suite of random affine transformation of the image keeping center invariant.</li>
      <li>RandomHorizontalFlip: horizontal flip of the image.</li>
      <li>ToTensor: convert the PIL image to a pytorch tensor.</li>
    </ol>
    <p>Note that, in every machine learning task we apply transformations only to the training set and not the
      validation or test set with exception such as normalization, converting to tensors. This is because we want to
      pretend that the validation &amp; test data are <strong>new, unseen data.</strong> We use the validation &amp;
      test dataset to get a good estimate of how our model performs on any new data.</p>
    <p>From <a href="https://qr.ae/pNKZAC">quora</a></p>
    <blockquote>
      <p>&quot;The training and testing data should undergo the same data preparation steps or the predictive model will
        not make sense. This means that the number of features for both the training and test set should be the same and
        represent the same thing. If your input is an image and you did pre-processing steps like resizing, background
        subtraction, normalization, etc, the same steps should be done to the test set. If you did data transformation,
        such as PCA, random projection etc, then the test set must be transformed using the parameters of the training
        set and then passed to the classifier.&quot;</p>
    </blockquote>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=20"
      title="Jovian Viewer" height="685" width="850" frameborder="0" scrolling="auto"></iframe>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=28"
      title="Jovian Viewer" height="744" width="850" frameborder="0" scrolling="auto"></iframe>
    <h3 class="mume-header" id="5-building-dataloader">5. Building Dataloader</h3>

    <p>Straight from the documentaion:</p>
    <blockquote>
      <p>Combines a dataset and a sampler, and provides an iterable over the given dataset.<br>
        The DataLoader supports both map-style and iterable-style datasets with single or multi-process loading,
        customizing loading order and optional automatic batching (collation) and memory pinning.</p>
    </blockquote>
    <p>We&apos;ll create 3 helper functions to fetch dataset and return dataloader for each set:</p>
    <ol>
      <li><strong>get_train_dataset(..)</strong></li>
      <li><strong>get_train_dataloader(..)</strong></li>
      <li><strong>get_test_dataloader(..)</strong></li>
    </ol>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=22"
      title="Jovian Viewer" height="685" width="850" frameborder="0" scrolling="auto"></iframe>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=23"
      title="Jovian Viewer" height="465" width="850" frameborder="0" scrolling="auto"></iframe>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=24"
      title="Jovian Viewer" height="565" width="850" frameborder="0" scrolling="auto"></iframe>
    <h3 class="mume-header" id="6-setup-gpu">6. Setup GPU</h3>

    <p>To use the power of GPU&apos;s we have to first move our data and models that we&apos;re going to create onto the
      availavble gpu.<br>
      We can do this with by:</p>
    <ol>
      <li>Create a recursive function <code>to_device</code> that iterates over the passed data and loads it into GPU
        memory.</li>
      <li>Create a class <code>DeviceDataLoader</code> that wraps our dataloader and uses the recursive function to move
        the data to GPU when accessed.</li>
    </ol>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=30"
      title="Jovian Viewer" height="625" width="850" frameborder="0" scrolling="auto"></iframe>
    <p>Get the current device in use:</p>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=31"
      title="Jovian Viewer" height="138" width="850" frameborder="0" scrolling="auto"></iframe>
    <h3 class="mume-header" id="7-model-building">7. Model Building</h3>

    <p>Finally, we&apos;ve arrived to the exiting part everyone likes, the <strong>Building models</strong>. For this
      project I used two models:</p>
    <ul>
      <li>A ResNet9 model</li>
      <li>Custom CNN model from scratch.</li>
    </ul>
    <p>We&apos;ll divide this section into two parts:</p>
    <ol>
      <li>Creating a BaseClass and metrics.</li>
      <li>Defining two seperate model classes: ResNet9 and custom CNN Model.</li>
    </ol>
    <h4 class="mume-header" id="71-parent-class-and-metrics">7.1 Parent Class and Metrics</h4>

    <p>Parent Class by definition means a class that contains some properties or methods that common to all the
      inhereting child classes. For our use case we&apos;ll create an <code>ImageClassificationBase</code> class that
      itself inherits from the <code>nn.Module</code> class from PyTorch. This class will contain some methods that can
      be used by the child classes such as:</p>
    <ol>
      <li><code>training_step</code> - method used to perform operations during the training step for the model.</li>
      <li><code>validation_step</code> - methods used to evaluate our model on the validation set.</li>
      <li><code>get_metrics_epoch_end</code> - contains operations that when called after training or validation step
        for a single epoch, returns the metrics such as <strong>accuracy</strong>, <strong>loss</strong>,
        <strong>val_accuracy</strong>, <strong>val_loss</strong>.</li>
      <li><code>epoch_end</code> - prints the current epoch metrics.</li>
    </ol>
    <p>The great thing about this class is that it can be used for any image classification problem in PyTorch with
      little to no updation necessary.</p>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=34"
      title="Jovian Viewer" height="745" width="850" frameborder="0" scrolling="auto"></iframe>
    <p><strong>Metric</strong>: Using the good old <code>accuracy</code>.</p>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=36"
      title="Jovian Viewer" height="145" width="850" frameborder="0" scrolling="auto"></iframe>
    <h4 class="mume-header" id="72-models">7.2 Models</h4>

    <h5 class="mume-header" id="721-resnet9">7.2.1 ResNet9</h5>

    <p>ResNets architectures by design uses skip connections in residual blocks, because of these skip connections, we
      can propagate larger gradients back to initial layers avoiding vanishing gradients and these layers also could
      learn as fast as the final layers, giving us the ability to train deeper networks.</p>
    <p>Here, I&apos;ve updated the default ResNet9 architecture to accomodate for the fact that we have a grayscale
      dataset.<br>
      We can simply do thid by updating the in and out channels in each <code>conv_block</code>, I&apos;ve also added an
      additional <code>Linear</code> layer classifier block.</p>
    <p><img src="https://raw.githubusercontent.com/lambdal/cifar10-fast/master/net.svg?sanitize=true" alt="Resnet9"><br>
      (open the image in a new tab and hover over the blocks to see how they are defined.)</p>
    <p>Building block of our ResNet9 architecture:</p>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=38"
      title="Jovian Viewer" height="305" width="850" frameborder="0" scrolling="auto"></iframe>
    <p>The <strong>ResNet9 model</strong> class:</p>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=39"
      title="Jovian Viewer" height="800" width="850" frameborder="0" scrolling="auto"></iframe>
    <h5 class="mume-header" id="721-custom-cnn-model">7.2.1 Custom CNN model</h5>

    <p>Let&apos;s build a <strong>CNN model</strong> to compare with the ResNet9 model</p>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=41"
      title="Jovian Viewer" height="800" width="850" frameborder="0" scrolling="auto"></iframe>
    <p>In both of these classes we can see a <code>forward</code> method. This method is called by the
      <code>training_step</code> method of the <em>BaseClass</em> using <code>self(inputs)</code>. This is because when
      we inherited the <code>nn.Module</code> class in the <em>BaseClass</em>, it expects the <code>forward</code>
      method in it to be overriden. Doing so provide us with the functionality to directly call the model (self) and
      pass it any arguments like <code>self(inputs)</code>, it&apos;ll internally call the model&apos;s forward method
      and pass it input parameters. We can then choose what we want to do within <code>forward</code> method as seen in
      both the <em>ResNet9</em> class and <em>EmotionRecognition</em> class.</p>
    <h3 class="mume-header" id="8-training-and-testing-helper-functions">8. Training and Testing Helper Functions</h3>

    <p>We&apos;re nearly at the end of project but first we need to define some helper functions to tie together all the
      above defined functions and classes.</p>
    <ol>
      <li>Create <code>fit_model</code> function which is reponsible for training and saving the best model.</li>
      <li>Create a <code>evaluate</code> function for evaluating the validation set.</li>
      <li>A <code>load_best</code> helper function to load our best model for evaluating the test set.</li>
      <li>A <code>generate_prediction</code> function for generating predictions on the test set using the best model.
      </li>
      <li>Plotting methods</li>
      <li>A <code>end-to-end</code> function that binds all these functions in a simple single call requiring the name
        of the model to use and training parameters as a dictionary.</li>
    </ol>
    <ul>
      <li><strong>Functions:</strong> <code>evaluate</code>, <code>fit_model</code></li>
    </ul>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=44"
      title="Jovian Viewer" height="800" width="850" frameborder="0" scrolling="auto"></iframe>
    <ul>
      <li><strong>Functions:</strong> <code>load_best</code> and <code>generate_predictions</code></li>
    </ul>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=45"
      title="Jovian Viewer" height="800" width="850" frameborder="0" scrolling="auto"></iframe>
    <ul>
      <li><strong>Functions:</strong> <code>plotting</code></li>
    </ul>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=47"
      title="Jovian Viewer" height="725" width="850" frameborder="0" scrolling="auto"></iframe>
    <ul>
      <li><strong>Functions:</strong> <code>end-to-end</code></li>
    </ul>
    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=46"
      title="Jovian Viewer" height="800" width="850" frameborder="0" scrolling="auto"></iframe>
    <h3 class="mume-header" id="9-training">9. Training</h3>

    <h4 class="mume-header" id="91-models">9.1 Models</h4>

    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=49"
      title="Jovian Viewer" height="165" width="850" frameborder="0" scrolling="auto"></iframe>
    <h4 class="mume-header" id="92-training-parameters">9.2 Training Parameters</h4>

    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=51"
      title="Jovian Viewer" height="228" width="850" frameborder="0" scrolling="auto"></iframe>
    <h4 class="mume-header" id="93-train-model">9.3 Train model</h4>

    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=52"
      title="Jovian Viewer" height="800" width="850" frameborder="0" scrolling="auto"></iframe>
    <h3 class="mume-header" id="10-training-plots">10. Training Plots</h3>

    <iframe
      src="https://jovian.ml/embed?url=https://jovian.ml/vaibhav-singh-3001/facial-expression-recognition/v/4&amp;cellId=54"
      title="Jovian Viewer" height="490" width="850" frameborder="0" scrolling="auto"></iframe>
    <h3 class="mume-header" id="11-tests">11. Tests</h3>

    <table>
      <thead>
        <tr>
          <th>model</th>
          <th>LR</th>
          <th>Test accuracy</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Emotion Recognition</td>
          <td>None</td>
          <td>0.659</td>
        </tr>
        <tr>
          <td>ResNet9 (current)</td>
          <td>None</td>
          <td>0.651</td>
        </tr>
        <tr>
          <td>Resnet9</td>
          <td>OneCycleLr</td>
          <td>0.327</td>
        </tr>
        <tr>
          <td>Emotion Recognition</td>
          <td>OneCycleLr</td>
          <td>0.26</td>
        </tr>
      </tbody>
    </table>
    <h3 class="mume-header" id="12-summary">12. Summary</h3>

    <p>We&apos;ve covered a lot of ground in this tutorial. Here&apos;s quick recap of the topics:</p>
    <ol>
      <li>Introduction to the Facial emotion recognition dataset for image classification.</li>
      <li>Prepared our dataset dataframe and created a custom <code>dataset</code> class.</li>
      <li>Applied Image augmentaions using <code>torch.transforms</code>.</li>
      <li>We created our own helper functions to for fetching datasets and creating dataloaders.</li>
      <li>We also created a wrapper <code>DeviceDataLoader</code> class for our <code>Dataloader</code> to move our data
        to the GPU.</li>
      <li>We looked at how to use the <code>nn.Module</code> class and also created a <code>BaseClass</code> that can be
        used for any image classification model.</li>
      <li>Created our own version of ResNet9 class and a custom CNN model for comparison.</li>
      <li>Created various helper functions to tie different components together.</li>
      <li>Finally we trained our model and plotted the training results.</li>
    </ol>

  </div>











</body>

</html>